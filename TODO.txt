BY PRIORITY (ISH)

    TODO
        - Simultaneous action
            + communication :/

        - profiling test

        - finish domain
        - create a method of sampling policies
        - create a teammate class for sampled policies
        - create the framework for the process
            + sample teammates
            + create expert set
            + plan online, updating teammate weights
            + communicate
                - EARLY TERMINATION CONDITION
                - exhaustive (small domain only?)
                - most disagreed on
                - most variance?
                - product of the two?
                - myopic
        - logging results
            + score
            + successful passes
            + unused resources
            + number of comms

    Places that traverse the policy graph:
        - graph_planner
            + create_node_set
            + prune
            + GraphNode.finite_horizon_string
        - modeling_agent
            + update_policy_graph
        - communication_scenario
            + traverse_policy_graph
            + compute_reachable_nodes
            + min_in_policy
            + max_out_of_policy
        - visualization
            + _add_edges
            + _edge_labels

    Old bugs?
        - Okay, often second round of comms shows graph where all nodes are 0 value
            - may occur because all states that can be queried have been pruned (yet are actions for some reason)
        - also, the graph viz thing fails a bunch
        - write paper
        - recalculate node values (and policy) in update_policy_graph_models
        - don't "get action" on root; just get the greedy action; or have update_policy_graph_models return the action
        - post comm, selects incorrect answer
        - equality issue when updating position in policy graph

    Paper notes:
        - Noa Agmon leading paper -> task is to find the best possible REACHABLE joint actions/plans
        - Definition of problem
            - decentralized MDP variant
            - the modeler's view (planning in world state x modeling space) / POMDP
                - focus on planning while learning, otherwise only vaguely justified
            - active vs passive learning
            - active learning in ad hoc teams -> sharing policy information
            - planning for active learning -> planning over policy knowledge
            - optimize not for prediction accuracy, but for utility maximization
            - applications within teams of multiple unknown agents
        - Theoretical aspects
            - conditions where policy sharing is necessary
            - combinatorial space
            - MDP structure
            - DAG
            - early termination
            - cost effect? - free comm -> perfect info -> perfect policy
            - not submodular; matroids don't help

    Considerations
        - Querying a state that is unreachable could inform many reachable policy states. Hmmm.
        - Subset queries aren't likely to be optimal (as query choice depends on answers), but can improve runtime and coverage
        - True EV of a policy is not submodular (no guarantees on greedy alg).
        - Matroids don't apply either, as responses matter.
        - It's still not quite clear how to handle global policy model vs policy instance
            + Narrowing a policy from a stochastic policy is probably easier than "learning", no? Just use info gain to discern.
                - Doesn't take into consideration utility and indifference.

    EARLY TERMINATION - technically optional, but possibly huge in impact.
        + remember, looking for total utility, not terminal utility. Harder. :/
        + find max outcome outside of policy
        + find min outcome within policy - antagonistic teammate, minimizing EV

    Thoughts on comm strategies:
        - Exact
            + Timing is a consideration. Don't technically need to comm now if policy
              only at risk of changing later. Can delay comms until then.
                - As such, storing where policy changes gives the comm timing requirements
                - Also consider not communicating max potential change, but max change in current NEED of policy change
        - Given conditional relations between state-action pairs of teammate model
            + select states with best inference power over all affected states
        - Greedy (consider each independently, as in last paper)
        - Best pruning (as queried action makes other actions' subtrees irrelevant)
        - Limit communication content: limited states considered (what to communicate paper)
        - Limit computation: Heuristic: Util x Info x likelihood of state
        - Limit search space: Limiting to similar payoffs
        - Alternate strategy (like conditional value at risk): Maximizing expected util vs raise lower bound on util (safety value).
        - Alternate strategy: don't max payoff; rather, max chance agent made correct call (between two alternative distributions, maximize
        probability sampled outcome of one is greater than sampled outcome of other). In other words, maximize instances of regret, not ev of regret.
        - Query by committee - pick states where set of models most disagree; can select N most likely models (N most likely paths?)
        - Expected model change - pick queries that may most change policy (expectation over answers)

    Domains
        Managing 2+ teammates simultaneously.
            Helps with reviewer criticism
        StarCraft
            Note: will have to figure out queue in scenario; also, caps for research (cannot keep researching same thing over and over)
            Evaluation? Battle simulator? Heuristic? Match to one or more possible goals?
        Small domains:
            Multi-armed bandit (introduce conflict in same bandit? shared obs? someone explores while the other exploits?)
            Cooperative flocking (various roles?)
            Blocks world (tagged blocks which can only be moved by certain agent)
            Traditional pursuit?
            Generated MDP (Noa Agmon's work "Leading Ad Hoc Agents...")
        Human supervised classification
            -> setup: model selects action classifying item in state. Successive state is human classifying image (can skip?).
            -> Reward: S A S' -> util when select correct classification
            -> May need to adjust how immediate values are handled by nodes during planning


Nice to have, but completely beyond the scope of the current project.
    Profiling and unit tests

    Cython
        MDP planner?
        Domain scenarios - state transitions are likely a major factor

    Debug
        - use logging module http://inventwithpython.com/blog/2012/04/06/stop-using-print-for-debugging-a-5-minute-quickstart-guide-to-pythons-logging-module/

    Multiprocessing
        - far advanced, but consider doing something parallel. You know, maybe you should do an initial search until X
        branches, where X = #processors, then have each process handle a subtree. That's interesting. Probably not a huge
        gain, however. Doesn't keep the philosophy of put more resources into Y process because it is more promising. As
        X -> inf, you just have breadth first search.

    Hierarchical Graph Reduction (maybe keep for second PhD :P)
        Epsilon merging
        Multi-level graph partitioning

