BY PRIORITY (ISH)

    State space coverage is increasingly a consideration for policy sampling strategies. The issue lies in the ratio
    of relevant states--those reachable and on the trajectory of a given policy--to irrelevant states (off-policy/unlikely
    states). The presence of irrelevant states greatly increases the variety of policies but at the cost of diluting
    information, as is the case in Cops and Robbers where ~10^47,000 unique policies exist, but only ~13 varieties of
    observed trajectories are at play. There's something to be said for meaningful differences in behavior via the
    likelihood and differences of the trajectories that result, not necessarily from the differences that exist at the
    state-action mapping level. I believe this falls under "behavior equivalence", as reported by one of Doshi's students,
    though I don't believe there was any really meaningful progress on that front in that small amount of work.

    We can, of course, alter how behaviors are sampled, cutting out fine-grained policy sampling for something that
    instead samples in the space of observable behaviors. We could forward sample trajectories of policies, perhaps
    only reachable states given one agent's fixed actions, and leave all other state-action pairs pruned out. This
    leads to yet another consideration for how the ad hoc agent models these predictions, then. We could leave such
    predictions unbound in the aggregate model, instead only relying on which agents HAVE visited such states.

    I think this tracks. Anything that would be off policy would have 0 probability anyway for arriving at the state.
    Therefore, it doesn't matter if they had one action or another. We can simply keep them as None or not contained at
    all (sparse data structure), which will help with equivalence. Thank god.

    So what does our CRP structure look like, then? I don't think we need to keep a dense matrix anymore. Instead of
    state index x policy index, we could do state -> sparse policy mapping. But what about queries? If we were to
    query a teammate about an off-policy state, what is the response? None? Do we sample an action? How does that
    affect the belief posterior? Anything we don't observe is updated with uniform posterior? Does that work? It doesn't
    rule it out, but it does bias the result a little more toward observed trajectories with the same action. Of course,
    that may be okay. How do we justify this, theoretically? Behavioral equivalence might cover this. We're really
    creating a model that covers all "behaviorally equivalent" model, as all other policy choices are irrelevant. It's
    kind of an abstraction over policies or a policy bucket.

    Ok, so the new process is as follows:
        - Sample only relevant states for policy. All other policy states are removed. (assume covered with enough coordination)
        - Policy likelihoods are updated uniformly for models of policies where no behavior was observed.
        - When querying a teammate, sample a full policy, so we can get real policy answers.
        - Do we add queried policy info to the table on update? I feel like we could (should?). It depends what we're testing.
            - If we're not learning on successive runs with queries, we can leave it off and just assume policy coverage.
            - If we are learning with successive queries, we should add individual models to the table.
            - We should make clear our process in the paper.



    ------

    It doesn't make sense to count the number of unique optimal policies over the entire state space, as the relevant
    or reachable portion of the state space is comparatively small. In a fully deterministic cops and robbers scenario
    with a horizon of X, only X states will ever be reached, despite a branching factor of 3-5 at every state.
        Horizon = 13
        States involved in optimal policies = 25 (1 shared root, 12x2 states beyond)

    Policy equality -> by state (reachable subgraph/states equal)
    Hierarchical policy?
    Store in its own graph structure?

    Heuristic specific to domain.

    Recipe sat scenario generates an ungodly number of optimal policies. I should try to constrain that.
    Interestingly enough, sampling out from the space only gave me like 1k unique policies. I need to check hashing
    and equality on those policies. Something seems off.

    I should see how many optimal policies exist for cops and robbers.
        Answer: soooooo many (8 x 10^47,238)




    Logging could likely be improved. I don't yet have a parser either for all of the information.
        Needs: read all files in directory into giant corpus.
        Pass arguments for search over records so we prune first, then aggregate.

    Comm search heuristic/end criteria:
        - The heuristic is an optimistic estimate of the total future change in the value of information.
            H(I) = max_{I', q} [ VOI(I') - VOI(I) - q * comm cost ]
        - VOI(I) is fixed (we've already calculated it)
        - q could be accurately estimated, if we had a sense of how many queries remained, but we know it's >= 1
        - This leaves VOI(I') as the main point of interest.
            VOI(I') = V(s0, π', I') - V(s0, π0, I')  # cannot easily be computed by DP, but we can bound it by
                    <= max_{π', I'} V(s0, π', I') - min V(s0, π0, I'')
        - if H(I) < 0, H(I) == 0, and we've reached our end criteria, as we can always stop at I with no further cost.

    New plan, don't save graphs!
    Calculate and cache EVERYTHING (in transition)
        the action set
        val of info = new policy ev - original policy ev
        end criterion

    Do it in transition once we compute the new policy states
        - util -> have all info necessary (new ev - orig ev | new info) - (prev ev - orig ev | pre info)
        - end -> cached
        - actions -> cached
    Delete actions from cache in actions call


    New optimizations (before I forget):
        Calculate new policy graph for s' in util(s,a,s') [IF VALUE OF INFO NOT IN CACHE]
            Cache model predictions and mapping to original policy graph during this process
            Cache depth map in root of graph
            Cache value of information for s' here, as we'll need it for ALL (s,a,s') calls
        Delete policy graph when actions(s') is called (upon expansion)
        Delete policy graph when end conditions reached (will never be expanded)


    Updates on debug/other
        Information states are Markovian as we assume the teammate's policy does not change.
        If information states are Markovian, so are policy updates and policy EVs.
        Observation:
            0 =  V(I2, π0) - V(I2, π1) + V(I1, π1) - V(I1, π0)  # in effect, going from policy A->B->A nets no value
            V(I2, π1) - V(I2, π0) = V(I1, π1) - V(I1, π0)       #


    CRP Policy
        Shared reference to policy matrix (saves a bunch of space)
        Individual models are then a probability distribution over indices of policies

        constructor -> takes reference to matrix as well as prob for uniform model
        predict(state) -> lookup dict (index -> action) for all indices; return distribution over actions
        update(state, action) -> go to state row; iterate over indices; delete 0 prob keys

    Model distribution functionality
        Keep probability vector, aligned with policy (is this necessary if policy is encoded in rows?)
        Keep state-wise action vector, also aligned with policy

        constructor ->
        predict(state) -> find state row; iterate over probs, actions; return distribution
        update(state, action) -> go to state row; iterate over, setting relevant probs to 0; delete 0 prob rows


    Models
        Recently I switched the project over to using Chinese Restaurant Processes for lifetime, population modeling,
        where the model keeps track of all observed policies seen so far and provides priors to the agent for individual
        trials. The prior includes a catch-all uniform policy teammate model, which represents all of the unobserved
        but potentially relevant teammate policies (which can be represented without exhaustively enumerating them).

        Due to the modeling needs of the scenario, the teammate model is represented as such:
            Communicating Teammate Model (stores and holds to previous policy commitments)
                Teammate Distribution Model - a distribution over
                    - Multiple OfflineSampledPolicyTeammate models (one policy each; also used for the actual teammate)
                    - One UniformPolicyTeammate model

    Thoughts on trajectories
        It is likely improper to model teammates as having no consideration of policy trajectory when selecting
        sub-policies, i.e. teammates likely consider other agents acting within the scene just as we are modeling
        them. That assumption only works in a very strict case: in a fully-observable multiagent MDP where all agents
        select policies identically.

        So where are we, then? An initial policy graph (tree, really) where every trajectory is represented.
            ~|A_j|^h            possible trajectories
            ~|A_j|^(h+1)        possible queries
            ~|A_m||A_j|^(h+1)   possible query-reply pairs at the communication policy root, |A_m| = teammate actions

        One benefit: each query response prunes |A_m|-1 subtrees from the policy graph. This benefit is great the closer
        to the current state the query trajectory state occurs. For example, querying the current state can effectively
        reduce the space of trajectories as well as the space of possible queries by a factor of |A_j|. In general,
        querying a trajectory state at time t < h reduces the space of trajectories to
            |A_j|^h - (|A_j| - 1) * |A_j|^(h-t-1)
        and the space of queries to
            |A_j|^(h+1) - (|A_j| - 1) * |A_j|^(h-t).    # check this

        Can we then reason about the space of query/information states? Pruning was not considered substation before,
        as we allowed for multiple trajectories to arrive at the same state, which meant pruning in an abstract sense
        was specific to the domain and never guaranteed. In this new perspective, pruning IS guaranteed, and should be
        considered. Take, for example, a top-down communication strategy. It can be clearly shown that it requires
        no more than h queries to reach a complete plan (assuming no stochasticity). From a bottom-up approach, the
        query space required is exhaustive (assuming no model update). The real savings, then, can come from model
        updates, as they have the potential to prune branches of possibility anywhere within the policy graph. But
        calculating this potential for pruning would require a global policy analysis, which again is computationally
        taxing. Active learning may have some method of predicting changes in global prediction variance which may be
        useful here. Otherwise, we are at best estimating that heuristics like local information entropy can have
        large impacts across the model's ability to predict the teammate policy accurately.

        So, in summary, we should select heuristics that
            (1) prune as much of the trajectory state space as possible
                - bias heuristics toward short term trajectories or at least trajectory states with large subtrees
                - consider a heuristic measure of expected pruning (we can do that now!)
            (2) affect the global potential for pruning by having large information changes within the predictive model
                - this is where heuristics like information entropy are relevant
            (3) consider the effect on expected utility
                - relevant for in highly stochastic environments, as pruning is less likely

        Variance in utility addresses (2) and (3) to some degree. λ^t can help with (1), though not in cases where λ=1.
        This may be an argument for including p(s|π), as it further narrows the search space and biases heuristic values
        to trajectory states in the near future. Of course, it ignores off-policy trajectories, but given a response
        that lowers the expected utility of a trajectory sufficiently, our policy would change, resulting in a new
        set of trajectory states.

        Does it make sense to consider a policy response to a trajectory that violates my personal policy, e.g. "what
        will you do if we do A->B->C?" when I don't plan to do B? Probably not, if my policy is fixed. Can we process
        a noop ("not in policy") response to our benefit? Yes. We can prune the entire policy subgraph from
        consideration, as we know that any fixed policy that leads to that trajectory state is invalid (drop those
        models). Model update: all policies leading to trajectory state are dropped (p(r|q)=0); all other policies are
        equally likely to have made that response (p(r|q)=1).

        Conversely, what happens if we query a trajectory and get a valid response? Does that confirm the current
        trajectory for the teammate policy? Do we set the probabilities of all models of opposing trajectories to 0?
        I'd think so, yes. And the remaining policies are all equally likely to generate the trajectory but not equally
        likely to generate the response.

        Given these two model update considerations, You could imagine a heuristic that selected trajectory states that
        most closely bisected the set/probabilities of the remaining teammate policies. Ex:
            Model   Expected Trajectory
            1       A-B-C
            2       A-B-C'
            3       A-B-C''
            4       A'-B-C
            5       A'-B'-C
            6       A''-B-C
                                Eliminations
            Possible queries    (in policy & r)  (not in policy)     E[elim]     p(r) log p(r)
            A                   3/6 (B,B,B)      3/6 (3/6)           18/36       3*1/6 log 1/6 + 3/6 log 3/6 = 1.2424533248940002*
            A'                  5/6 (B,B')       2/6 (4/6)           18/36       2*1/6 log 1/6 + 4/6 log 4/6 = 0.8675632284814612
            A-B                 5/6 (C,C',C'')   3/6 (3/6)           24/36*      3*1/6 log 1/6 + 3/6 log 3/6 = 1.2424533248940002*
            A'-B                5/6 (C)          1/6 (5/6)           10/36       1/6 log 1/6 + 5/6 log 5/6   = 0.45056120886630463
            A'-B'               5/6 (C)          1/6 (5/6)           10/36       1/6 log 1/6 + 5/6 log 5/6   = 0.45056120886630463

            for policy in policies:
                response = get_response(policy)
                probs[response] += pr(policy)       # response = (Yes, action) or (No)

            Exp[elim] = sum(prob * (1-prob) for prob in probs.values)




    TODO
        Code heuristics
        Make sure test runs work

        Set up logging and test
        Set up experiments

        Set up virtual env code for cluster
        Submit jobs

        While experiments are running,
            Work on other learning algs
            Develop other test domains
            Begin paper

    On-policy vs off-policy
        Same if policy is greedy (argmax a) and updated each cycle, so there really isn't a distinction here.
        Off-policy - Q(s,a) <- Q(s,a) + c * (r(s) + y max_a' Q(s', a') - Q(s,a))  # Bellman backup      (estimates value of best policy)
        On-policy -  Q(s,a) <- Q(s,a) + c * (r(s) + y * Q(s', pi(s'))  - Q(s,a))  # Fixed-policy backup (estimates value of given policy)

    Variance in util not straightforward without MY policy probs
        New heuristics!
            - local entropy minimization - minimize p log p   (can be weighted by p(s))
            - global entropy minimization - pick query to minimize p log p across entire graph (weighted by p(s)); needs to update entire policy graph

            - local value of information - consider value increase at the state
                sum p(a) (p'(s_t|a)V'(s_t|a) - p(s_t|a)V(s_t|a))   # p'(s_t) is dependent on policy from 0 to t-1 (not markovian), but V'(s_t)|a isn't. Two options:
                    1. p'(s_t|a) ~= p(s_t|a) -> p(s_t) * sum p(a) (V'(s_t|a) - V(s_t|a))  # (0 if off policy!) how important it'd be if we were there, weighted by current likelihood
                    2. p'(s_t|a) <= 1        -> sum p(a) (V'(s_t|a) - p(s_t|a)V(s_t|a))   # (!0 if off policy!) upper bound on the change of value this could contribute to V(s_0)
            - global value of information - myopic query, then update entire policy

            - local expected quadratic loss - linear loss is 0, no? [sum p(a) V(actual)] - V(expected) = V(expected) - V(expected)
                p(s_t) sum p(a) (V(actual) - V(expected))^2   # p(s_t) or (p(s_t) - 0) is an upper bound on the potential change in prob of bad prediction (emphasizes on-policy, minimizes off-policy)
            - global expected quadratic loss - query then update entire policy, calculate weighted sum of quadratic loss across entire graph

            - could use a linear loss if it is an absolute loss, ie p * | V' - V |

            - consider: what about asymmetric loss functions? We use quadratic instead of linear (because expected linear loss = 0), but what about losses that weight underestimations and
            overestimations differently? In our case, underestimating the chance of a teammate doing something unexpected or against our policy can be far worse than underestimating that
            they WILL do what we want (in which case, communication could be skipped in certain circumstances).

            - could also consider losses/metrics based on distributions (e.g. KL-divergence) over local state transitions, local action probs,
            global teammate policy likelihoods, global state likelihoods, global outcome likelihoods, etc
            - in this thinking, cross entropy seems like a great candidate, as it is used when predicting a prob dist but truth is all 0/1
                -> in general, cross entropy is preferred for classification problems (we're classifying policies!) while MSE is better for regression (we're also learning distributions over policies!)




    Each heuristic should be given the model state (in comm planner, not in node) and world state to predict on
        things got crossed when using "node" as parameter

    Justification for uniform policy distribution
        - care equally about underestimating teammate's policy (out of policy) as overestimating (in-policy) the teammate (which disappoints us)
        - if probabilities are biased toward policy, swings in util where incorrect prediction occurs have little weight
        - in truth, (max prob of state - current prob of state) would give a better estimate, but max prob is expensive to calculate
        - result of uniform -> highly stochastic transitions lead to unlikely states (which is fair, as we'd then care more about near-term predictions)



    Quick notes
        node_likelihoods
            get_active_node_set
            weighted_entropy
            weighted_variance
        get_active_node_set
            each comm strategy
            each comm loop
            coordinated_actions -> heuristic_comm

        !!! too much redundant calculation !!!

        process should be:
            main loop plans
            main loop calls function that sets up comm scenario, does search, and returns a query (communicate()?)
            comm loop plans
            comm scenario actions() calls node_likelihoods
            comm scenario actions() passes through series of filters to get universal eligible state set (eg no p=0 states given non-zero likelihood for all our policy decisions)

            comm scenario actions() calls heuristic technique to rate queries, then heapq nlargest to get the top picks
            comm loop completes and returns query
            main loop queries, updates model, takes action, and repeats

    check experiments
        currently it uses run_comm_attempt which just takes the top heuristic eval as the comm
            maybe use this as comparison?
        need to use communicate() from communication_scenario, as it uses the planner to plan comms


    brainstorming on scenario with one composite policy (actions and comms planned together)
    new scenario would have the following changes:
        actions now includes query actions (no state change, but model change (part of state rep))
            except when world state denies it (agents in separate places, for instance)
        early termination no longer holds, as it may need to query still to ensure good policy in future states where comms prevented
        early termination, then, is really just a pruning eval for mixed scenarios with completely open comms
        in open comms, prove continued comms end value <= early termination end value
            basically show at that point, you've settled on the immediate action
                if the action result is deterministic, they are equal in resulting value
                otherwise, possibly save on comms from stochastic transition (avoid a state)
                (only works for constant comm cost)
        and the branching factor together makes the mdp just so big!
        and it doesn't have to be comms/no comms, really you could adjust comm cost (high cost = impossible)



    TODO
        - beyond 10 (total visits and action counts), the exploration term changes only by a factor of [0.98, 1.0]
            Can probably short cut UCT calc by caching value at 10, then multiplying by 0.99 at each step
            e.g. (future_value + 1 ) * cached_value * 0.99  (then update cached value to be * 0.99, or do that first)
        - add pruning/heuristic selection to algorithm (comm scenario.actions() (only consider top 5, etc))
        - add early termination to end() method of comm scenario
            - remember we're early terminating for the current line of queries (for the current action), not for all queries
        - comm search heuristic?
            - want to estimate the gain gotten by optimally querying/terminating
        - go over communication process code (in comm scenario)
        - add logging from new log_config json support
        - run small test
        - go over experimental setup
        - finalize changes and run first test!!!

    Tasks:
        - SET UP ENV TO INCLUDE logmatic-python (for logging in JSON!)
        - comms
            - test comm heuristics
            - full comm alg? pruned dec theory? YES

        - recipe sat
            - test domain to fix parameters
                - num recipes
                - num conditions
                - max length

        - experimental setups
            - vary cost of comm
            - vary cost of conflict
            - vary prior knowledge
            - vary num trials with same team (pre-comm)
            - vary heuristics
            - vary comm action space (with heuristics)
            - improving outcome for suboptimal team policy? (like Noa's work)
            - multiple teammates

        - planning optimization?
            - can attempt backup operator optimization with little risk

    Data to log:
        Meta data (not operational data)
            - Parameters
                + Comm cost
                + Heuristic
                + Branching limit
                + Graph size (comm and policy)
                + Prior knowledge
            - Trial number
            - Process info?
        Per run
            Per step
                - Expected util pre comm
                - Each comm
                - Each expected util after comm



    How:
        Docs: https://docs.python.org/3/howto/logging.html
        Cookbook: https://docs.python.org/3/howto/logging-cookbook.html
        Custom formatting: https://stackoverflow.com/questions/14844970/modifying-logging-message-format-based-on-message-logging-level-in-python3
        BaseRotatingHandler -> custom subclass to "rotate" between files (for different runs?)
        JSON logging: https://logmatic.io/blog/python-logging-with-json-steroids/
                      https://doc.logmatic.io/docs/python
        Tiers:
            - Debug: Dump any message (little/no format). Can be used for params (init) and odd outputs for... debugging.
            - Info: Reserved strictly for computational output.
            - Warning?
            - Error: For errors.
            - Critical?
        Handlers -
        Filters - each handler (and logger!) can filter by record level (warning/debug/etc) https://stackoverflow.com/questions/1383254/logging-streamhandler-and-standard-streams

    OSG Process:
        fabfile
            - Connects via ssh into OSG
            - Logs in with account name and rsa key
            - Runs various Linux commands
                + git init, pull, mkdir for logs
                + condor_submit osg_setup.submit
                + waits, then pulls files in output folder (move to separate method/fab call)
        submit file
            - specifies criteria for deploying computation
                + shell file to run
                + output, error, log destinations
                + requests for computers, memory, disk space
                + specifies command line args to each individual process
        shell file
            - loads python, unpacks virtual env and files
            - mkdir for output
            - runs python script with arguments
            - compresses output


    Desires for experimental domain:
        - Multiple policies are successful
        - Scalable
        - Rearrangeable
            + add or remove policy dependencies
        - clear way to author/sample/learn teammate policies


    OSG Structure
        - fabfile logs in and performs remote commands
        - submit file specifies requirements for computation in condor network as well as what to run (.sh file)
        - .sh file loads modules/copies data and runs actual python files, converting output


    Experiment goals (Show ____):
        - exhaustive good, but infeasible
        - myopic bad
        - entropy okay (p log p)
        - variance in util better (hopefully)  (E[U' - E[U]])
        - weighting by likelihood (either) -> best   Ps (p log p)   ///   Ps (E[U' - E[U]])   (Ps = prob state)
        - if we had a paramaterized model, we could use Fisher information, but we don't (thank god)

        - more experience -> less comms?

    Active learning:
        - Fisher information? - dependent on model parameters (which we're not using a parameterized model)
        - Query by committee
            - useful for set of prior models
            - also works on N best (most likely) sequence labelings (using model predictions, generate the top most likely outcomes, use that as committee)

    Paper notes:
        - Search through hypothesis space. V subset of H - version space - set of hypotheses which are
            consistent with the observed data. See Burr Settles stuff on active learning.
        - Similar to minimizing expected error of prediction (forward looking through queries), but with a diff objective function
        - Noa Agmon leading paper -> task is to find the best possible REACHABLE joint actions/plans
        - Definition of problem
            - decentralized MDP variant
            - the modeler's view (planning in world state x modeling space) / POMDP
                - focus on planning while learning, otherwise only vaguely justified
            - active vs passive learning
            - active learning in ad hoc teams -> sharing policy information
            - planning for active learning -> planning over policy knowledge
            - optimize not for prediction accuracy, but for utility maximization
            - applications within teams of multiple unknown agents
        - Theoretical aspects
            - conditions where policy sharing is necessary
            - combinatorial space
            - MDP structure
            - DAG
            - early termination
            - cost effect? - free comm -> perfect info -> perfect policy
            - not submodular; matroids don't help

    Considerations
        - Querying a state that is unreachable could inform many reachable policy states. Hmmm.
        - Subset queries aren't likely to be optimal (as query choice depends on answers), but can improve runtime and coverage
        - True EV of a policy is not submodular (no guarantees on greedy alg).
        - Matroids don't apply either, as responses matter.
        - It's still not quite clear how to handle global policy model vs policy instance
            + Narrowing a policy from a stochastic policy is probably easier than "learning", no? Just use info gain to discern.
                - Doesn't take into consideration utility and indifference.

    EARLY TERMINATION - technically optional, but possibly huge in impact.
        + remember, looking for total utility, not terminal utility. Harder. :/
        + find max outcome outside of policy
        + find min outcome within policy - antagonistic teammate, minimizing EV

    Thoughts on comm strategies:
        - Exact
            + Timing is a consideration. Don't technically need to comm now if policy
              only at risk of changing later. Can delay comms until then.
                - As such, storing where policy changes gives the comm timing requirements
                - Also consider not communicating max potential change, but max change in current NEED of policy change
        - Given conditional relations between state-action pairs of teammate model
            + select states with best inference power over all affected states
        - Greedy (consider each independently, as in last paper)
        - Best pruning (as queried action makes other actions' subtrees irrelevant)
        - Limit communication content: limited states considered (what to communicate paper)
        - Limit computation: Heuristic: Util x Info x likelihood of state
        - Limit search space: Limiting to similar payoffs
        - Alternate strategy (like conditional value at risk): Maximizing expected util vs raise lower bound on util (safety value).
        - Alternate strategy: don't max payoff; rather, max chance agent made correct call (between two alternative distributions, maximize
        probability sampled outcome of one is greater than sampled outcome of other). In other words, maximize instances of regret, not ev of regret.
        - Query by committee - pick states where set of models most disagree; can select N most likely models (N most likely paths?)
        - Expected model change - pick queries that may most change policy (expectation over answers)

    Domains
        Managing 2+ teammates simultaneously.
            Helps with reviewer criticism
        Small domains:
            Multi-armed bandit (introduce conflict in same bandit? shared obs? someone explores while the other exploits?)
            Cooperative flocking (various roles?)
            Blocks world (tagged blocks which can only be moved by certain agent)
            Traditional pursuit?
            Generated MDP (Noa Agmon's work "Leading Ad Hoc Agents...")
        Human supervised classification
            -> setup: model selects action classifying item in state. Successive state is human classifying image (can skip?).
            -> Reward: S A S' -> util when select correct classification
            -> May need to adjust how immediate values are handled by nodes during planning


Nice to have, but completely beyond the scope of the current project.
    Profiling and unit tests

    Cython
        MDP planner?
        Domain scenarios - state transitions are likely a major factor

    Debug
        - use logging module http://inventwithpython.com/blog/2012/04/06/stop-using-print-for-debugging-a-5-minute-quickstart-guide-to-pythons-logging-module/

    Multiprocessing
        - far advanced, but consider doing something parallel. You know, maybe you should do an initial search until X
        branches, where X = #processors, then have each process handle a subtree. That's interesting. Probably not a huge
        gain, however. Doesn't keep the philosophy of put more resources into Y process because it is more promising. As
        X -> inf, you just have breadth first search.

    Hierarchical Graph Reduction (maybe keep for second PhD :P)
        Epsilon merging
        Multi-level graph partitioning


Modeling Notes:

    An interface for an agent model which can make state-based predictions as well as update with new information.

    We can take the state + operators approach. It works for state because all scenarios have a similar underlying
    representation: state features and values. But a model?

    Is it just a state-action mapping? No. It's a state x obs history -> action mapping. Just like a POMDP.
    Is dynamic programming relevant here, as with belief states?

    Is it a calculation? It can be, but it may be potentially computationally intensive.

    Should we cache the already computed components? How does a model update affect cached results?

    Is a model Markovian? 30 heads/70 tails -> 30% prediction, but 3/7 is updated very differently.

    In effect, a model is a distribution over possible policy instances. Communication highlights which instance we are in.