BY PRIORITY (ISH)

    TODO
        - go over algorithm code
        - go over experimental setup
        - run small test
        - finalize changes and run

    May tasks:
        - comms
            - test comm heuristics
            - decide on stopping criteria (dec theory?)
            - full comm alg? pruned dec theory?
        - recipe sat
            - test domain to fix parameters
                - num recipes
                - num conditions
                - max length

        - experimental setups
            - vary cost of comm
            - vary cost of conflict
            - vary prior knowledge
            - vary num trials with same team (pre-comm)
            - vary heuristics
            - vary comm action space (with heuristics)
            - improving outcome for suboptimal team policy? (like Noa's work)
            - multiple teammates
        - planning optimization

    Planning note:
        - >1/3 of the planning time is spent calculating action values (during backup)
        - can reduce planning time by only completing a backup policy calculation every X iterations
        - in theory, this could be a huge speedup without losing asymptotic convergence

        - traverse nodes - incomplete successors

    Desires for experimental domain:
        - Multiple policies are successful
        - Scalable
        - Rearrangeable
            + add or remove policy dependencies
        - clear way to author/sample/learn teammate policies


    OSG Structure
        - fabfile logs in and performs remote commands
        - submit file specifies requirements for computation in condor network as well as what to run (.sh file)
        - .sh file loads modules/copies data and runs actual python files, converting output


    Experiment goals (Show ____):
        - exhaustive good, but infeasible
        - myopic bad
        - entropy okay
        - variance better (hopefully)
        - weighting by likelihood (either) -> best

        - more experience -> less comms?

    Active learning:
        - Fisher information? - dependent on model parameters
        - Query by committee
            - useful for set of prior models
            - also works on N best (most likely) sequence labelings (using model predictions, generate the top most likely outcomes, use that as committee)


    Places that traverse the policy graph: (why did I make this list?)
        - graph_planner
            + create_node_set
            + prune
            + GraphNode.finite_horizon_string
        - modeling_agent
            + update_policy_graph
        - communication_scenario
            + traverse_policy_graph
            + compute_reachable_nodes
            + min_in_policy
            + max_out_of_policy
        - visualization
            + _add_edges
            + _edge_labels


    Paper notes:
        - Noa Agmon leading paper -> task is to find the best possible REACHABLE joint actions/plans
        - Definition of problem
            - decentralized MDP variant
            - the modeler's view (planning in world state x modeling space) / POMDP
                - focus on planning while learning, otherwise only vaguely justified
            - active vs passive learning
            - active learning in ad hoc teams -> sharing policy information
            - planning for active learning -> planning over policy knowledge
            - optimize not for prediction accuracy, but for utility maximization
            - applications within teams of multiple unknown agents
        - Theoretical aspects
            - conditions where policy sharing is necessary
            - combinatorial space
            - MDP structure
            - DAG
            - early termination
            - cost effect? - free comm -> perfect info -> perfect policy
            - not submodular; matroids don't help

    Considerations
        - Querying a state that is unreachable could inform many reachable policy states. Hmmm.
        - Subset queries aren't likely to be optimal (as query choice depends on answers), but can improve runtime and coverage
        - True EV of a policy is not submodular (no guarantees on greedy alg).
        - Matroids don't apply either, as responses matter.
        - It's still not quite clear how to handle global policy model vs policy instance
            + Narrowing a policy from a stochastic policy is probably easier than "learning", no? Just use info gain to discern.
                - Doesn't take into consideration utility and indifference.

    EARLY TERMINATION - technically optional, but possibly huge in impact.
        + remember, looking for total utility, not terminal utility. Harder. :/
        + find max outcome outside of policy
        + find min outcome within policy - antagonistic teammate, minimizing EV

    Thoughts on comm strategies:
        - Exact
            + Timing is a consideration. Don't technically need to comm now if policy
              only at risk of changing later. Can delay comms until then.
                - As such, storing where policy changes gives the comm timing requirements
                - Also consider not communicating max potential change, but max change in current NEED of policy change
        - Given conditional relations between state-action pairs of teammate model
            + select states with best inference power over all affected states
        - Greedy (consider each independently, as in last paper)
        - Best pruning (as queried action makes other actions' subtrees irrelevant)
        - Limit communication content: limited states considered (what to communicate paper)
        - Limit computation: Heuristic: Util x Info x likelihood of state
        - Limit search space: Limiting to similar payoffs
        - Alternate strategy (like conditional value at risk): Maximizing expected util vs raise lower bound on util (safety value).
        - Alternate strategy: don't max payoff; rather, max chance agent made correct call (between two alternative distributions, maximize
        probability sampled outcome of one is greater than sampled outcome of other). In other words, maximize instances of regret, not ev of regret.
        - Query by committee - pick states where set of models most disagree; can select N most likely models (N most likely paths?)
        - Expected model change - pick queries that may most change policy (expectation over answers)

    Domains
        Managing 2+ teammates simultaneously.
            Helps with reviewer criticism
        Small domains:
            Multi-armed bandit (introduce conflict in same bandit? shared obs? someone explores while the other exploits?)
            Cooperative flocking (various roles?)
            Blocks world (tagged blocks which can only be moved by certain agent)
            Traditional pursuit?
            Generated MDP (Noa Agmon's work "Leading Ad Hoc Agents...")
        Human supervised classification
            -> setup: model selects action classifying item in state. Successive state is human classifying image (can skip?).
            -> Reward: S A S' -> util when select correct classification
            -> May need to adjust how immediate values are handled by nodes during planning


Nice to have, but completely beyond the scope of the current project.
    Profiling and unit tests

    Cython
        MDP planner?
        Domain scenarios - state transitions are likely a major factor

    Debug
        - use logging module http://inventwithpython.com/blog/2012/04/06/stop-using-print-for-debugging-a-5-minute-quickstart-guide-to-pythons-logging-module/

    Multiprocessing
        - far advanced, but consider doing something parallel. You know, maybe you should do an initial search until X
        branches, where X = #processors, then have each process handle a subtree. That's interesting. Probably not a huge
        gain, however. Doesn't keep the philosophy of put more resources into Y process because it is more promising. As
        X -> inf, you just have breadth first search.

    Hierarchical Graph Reduction (maybe keep for second PhD :P)
        Epsilon merging
        Multi-level graph partitioning


Modeling Notes:

    An interface for an agent model which can make state-based predictions as well as update with new information.

    We can take the state + operators approach. It works for state because all scenarios have a similar underlying
    representation: state features and values. But a model?

    Is it just a state-action mapping? No. It's a state x obs history -> action mapping. Just like a POMDP.
    Is dynamic programming relevant here, as with belief states?

    Is it a calculation? It can be, but it may be potentially computationally intensive.

    Should we cache the already computed components? How does a model update affect cached results?

    Is a model Markovian? 30 heads/70 tails -> 30% prediction, but 3/7 is updated very differently.

    Eventually, the MDP planner will run with state being scenario state + current model. It needs to be hashable.

    Other goal: override components with communication. Single instant commitments + multiple trial trends.

    In effect, a model is a distribution over possible instances. Communication highlights which instant we are in.
    A partially observable case with observations in two forms.

    Update instance model with every observation/communication.
    Update global model... between trials?

    There are dependent components and conditionally independent components. A factored model?

    Multiple issues here:
        - Deterministic teammate or probabilistic
            + obs/comms are gold truths for deterministic
            + obs/comms are instances of possible plans in probabilistic
        - Global vs individual model
            + global model can be updated later
            + individual model should be updated online

    What should I choose?
        - I like the idea of stochastic models
        - general model to individual is some weird interpolation

    Global model -> a distribution over potential instances of behavior   (a prior)
    Observations and communication -> adjust probabilities, improving expectation (?)
    Learning -> what is the association between the global model and the individual model?

    A big problem is what to do when an agent is different from anything seen, when it defies expectation.

    What we want:
        - when behavior aligns with known instances: converge to plan
        - when behavior is different: remain uncertain (let comm handle it)
