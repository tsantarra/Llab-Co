BY PRIORITY (ISH)



    On-policy vs off-policy
        Same if policy is greedy (argmax a) and updated each cycle, so there really isn't a distinction here.
        Off-policy - Q(s,a) <- Q(s,a) + c * (r(s) + y max_a' Q(s', a') - Q(s,a))  # Bellman backup      (estimates value of best policy)
        On-policy -  Q(s,a) <- Q(s,a) + c * (r(s) + y * Q(s', pi(s'))  - Q(s,a))  # Fixed-policy backup (estimates value of given policy)

    Variance in util not straightforward without MY policy probs
        New heuristics!
            - local entropy minimization - minimize p log p   (can be weighted by p(s))
            - globabl entropy minimization - pick query to minimize p log p across entire graph (weighted by p(s)); needs to update entire policy graph

            - local value of information - consider value increase at the state
                sum p(a) (p'(s_t|a)V'(s_t|a) - p(s_t|a)V(s_t|a))   # p'(s_t) is dependent on policy from 0 to t-1 (not markovian), but V'(s_t)|a isn't. Two options:
                    1. p'(s_t|a) ~= p(s_t|a) -> p(s_t) * sum p(a) (V'(s_t|a) - V(s_t|a))  # (0 if off policy!) how important it'd be if we were there, weighted by current likelihood
                    2. p'(s_t|a) <= 1        -> sum p(a) (V'(s_t|a) - p(s_t|a)V(s_t|a))   # (!0 if off policy!) upper bound on the change of value this could contribute to V(s_0)
            - global value of information - myopic query, then update entire policy

            - local expected quadratic loss - linear loss is 0, no? [sum p(a) V(actual)] - V(expected) = V(expected) - V(expected)
                p(s_t) sum p(a) (V(actual) - V(expected))^2   # p(s_t) or (p(s_t) - 0) is an upper bound on the potential change in prob of bad prediction (emphasizes on-policy, minimizes off-policy)
            - global expected quadratic loss - query then update entire policy, calculate weighted sum of quadratic loss across entire graph

    Each heuristic should be given the model state (in comm planner, not in node) and world state to predict on
        things got crossed when using "node" as parameter

    Justification for uniform policy distribution
        - care equally about underestimating teammate's policy (out of policy) as overestimating (in-policy) the teammate (which disappoints us)
        - if probabilities are biased toward policy, swings in util where incorrect prediction occurs have little weight
        - in truth, (max prob of state - current prob of state) would give a better estimate, but max prob is expensive to calculate
        - result of uniform -> highly stochastic transitions lead to unlikely states (which is fair, as we'd then care more about near-term predictions)



    Quick notes
        node_likelihoods
            get_active_node_set
            weighted_entropy
            weighted_variance
        get_active_node_set
            each comm strategy
            each comm loop
            coordinated_actions -> heuristic_comm

        !!! too much redundant calculation !!!

        process should be:
            main loop plans
            main loop calls function that sets up comm scenario, does search, and returns a query (communicate()?)
            comm loop plans
            comm scenario actions() calls node_likelihoods
            comm scenario actions() passes through series of filters to get universal eligible state set (eg no p=0 states given non-zero likelihood for all our policy decisions)

            comm scenario actions() calls heuristic technique to rate queries, then heapq nlargest to get the top picks
            comm loop completes and returns query
            main loop queries, updates model, takes action, and repeats

    check experiments
        currently it uses run_comm_attempt which just takes the top heuristic eval as the comm
            maybe use this as comparison?
        need to use communicate() from communication_scenario, as it uses the planner to plan comms


    brainstorming on scenario with one composite policy (actions and comms planned together)
    new scenario would have the following changes:
        actions now includes query actions (no state change, but model change (part of state rep))
            except when world state denies it (agents in separate places, for instance)
        early termination no longer holds, as it may need to query still to ensure good policy in future states where comms prevented
        early termination, then, is really just a pruning eval for mixed scenarios with completely open comms
        in open comms, prove continued comms end value <= early termination end value
            basically show at that point, you've settled on the immediate action
                if the action result is deterministic, they are equal in resulting value
                otherwise, possibly save on comms from stochastic transition (avoid a state)
                (only works for constant comm cost)
        and the branching factor together makes the mdp just so big!
        and it doesn't have to be comms/no comms, really you could adjust comm cost (high cost = impossible)



    TODO
        - beyond 10 (total visits and action counts), the exploration term changes only by a factor of [0.98, 1.0]
            Can probably short cut UCT calc by caching value at 10, then multiplying by 0.99 at each step
            e.g. (future_value + 1 ) * cached_value * 0.99  (then update cached value to be * 0.99, or do that first)
        - add pruning/heuristic selection to algorithm (comm scenario.actions() (only consider top 5, etc))
        - add early termination to end() method of comm scenario
            - remember we're early terminating for the current line of queries (for the current action), not for all queries
        - comm search heuristic?
            - want to estimate the gain gotten by optimally querying/terminating
        - go over communication process code (in comm scenario)
        - add logging from new log_config json support
        - run small test
        - go over experimental setup
        - finalize changes and run first test!!!

    Tasks:
        - SET UP ENV TO INCLUDE logmatic-python (for logging in JSON!)
        - comms
            - test comm heuristics
            - full comm alg? pruned dec theory? YES

        - recipe sat
            - test domain to fix parameters
                - num recipes
                - num conditions
                - max length

        - experimental setups
            - vary cost of comm
            - vary cost of conflict
            - vary prior knowledge
            - vary num trials with same team (pre-comm)
            - vary heuristics
            - vary comm action space (with heuristics)
            - improving outcome for suboptimal team policy? (like Noa's work)
            - multiple teammates

        - planning optimization?
            - can attempt backup operator optimization with little risk

    Data to log:
        Meta data (not operational data)
            - Parameters
                + Comm cost
                + Heuristic
                + Branching limit
                + Graph size (comm and policy)
                + Prior knowledge
            - Trial number
            - Process info?
        Per run
            Per step
                - Expected util pre comm
                - Each comm
                - Each expected util after comm



    How:
        Docs: https://docs.python.org/3/howto/logging.html
        Cookbook: https://docs.python.org/3/howto/logging-cookbook.html
        Custom formatting: https://stackoverflow.com/questions/14844970/modifying-logging-message-format-based-on-message-logging-level-in-python3
        BaseRotatingHandler -> custom subclass to "rotate" between files (for different runs?)
        JSON logging: https://logmatic.io/blog/python-logging-with-json-steroids/
                      https://doc.logmatic.io/docs/python
        Tiers:
            - Debug: Dump any message (little/no format). Can be used for params (init) and odd outputs for... debugging.
            - Info: Reserved strictly for computational output.
            - Warning?
            - Error: For errors.
            - Critical?
        Handlers -
        Filters - each handler (and logger!) can filter by record level (warning/debug/etc) https://stackoverflow.com/questions/1383254/logging-streamhandler-and-standard-streams

    OSG Process:
        fabfile
            - Connects via ssh into OSG
            - Logs in with account name and rsa key
            - Runs various Linux commands
                + git init, pull, mkdir for logs
                + condor_submit osg_setup.submit
                + waits, then pulls files in output folder (move to separate method/fab call)
        submit file
            - specifies criteria for deploying computation
                + shell file to run
                + output, error, log destinations
                + requests for computers, memory, disk space
                + specifies command line args to each individual process
        shell file
            - loads python, unpacks virtual env and files
            - mkdir for output
            - runs python script with arguments
            - compresses output


    Desires for experimental domain:
        - Multiple policies are successful
        - Scalable
        - Rearrangeable
            + add or remove policy dependencies
        - clear way to author/sample/learn teammate policies


    OSG Structure
        - fabfile logs in and performs remote commands
        - submit file specifies requirements for computation in condor network as well as what to run (.sh file)
        - .sh file loads modules/copies data and runs actual python files, converting output


    Experiment goals (Show ____):
        - exhaustive good, but infeasible
        - myopic bad
        - entropy okay (p log p)
        - variance in util better (hopefully)  (E[U' - E[U]])
        - weighting by likelihood (either) -> best   Ps (p log p)   ///   Ps (E[U' - E[U]])   (Ps = prob state)
        - if we had a paramaterized model, we could use Fisher information, but we don't (thank god)

        - more experience -> less comms?

    Active learning:
        - Fisher information? - dependent on model parameters (which we're not using a parameterized model)
        - Query by committee
            - useful for set of prior models
            - also works on N best (most likely) sequence labelings (using model predictions, generate the top most likely outcomes, use that as committee)

    Paper notes:
        - Search through hypothesis space. V subset of H - version space - set of hypotheses which are
            consistent with the observed data. See Burr Settles stuff on active learning.
        - Similar to minimizing expected error of prediction (forward looking through queries), but with a diff objective function
        - Noa Agmon leading paper -> task is to find the best possible REACHABLE joint actions/plans
        - Definition of problem
            - decentralized MDP variant
            - the modeler's view (planning in world state x modeling space) / POMDP
                - focus on planning while learning, otherwise only vaguely justified
            - active vs passive learning
            - active learning in ad hoc teams -> sharing policy information
            - planning for active learning -> planning over policy knowledge
            - optimize not for prediction accuracy, but for utility maximization
            - applications within teams of multiple unknown agents
        - Theoretical aspects
            - conditions where policy sharing is necessary
            - combinatorial space
            - MDP structure
            - DAG
            - early termination
            - cost effect? - free comm -> perfect info -> perfect policy
            - not submodular; matroids don't help

    Considerations
        - Querying a state that is unreachable could inform many reachable policy states. Hmmm.
        - Subset queries aren't likely to be optimal (as query choice depends on answers), but can improve runtime and coverage
        - True EV of a policy is not submodular (no guarantees on greedy alg).
        - Matroids don't apply either, as responses matter.
        - It's still not quite clear how to handle global policy model vs policy instance
            + Narrowing a policy from a stochastic policy is probably easier than "learning", no? Just use info gain to discern.
                - Doesn't take into consideration utility and indifference.

    EARLY TERMINATION - technically optional, but possibly huge in impact.
        + remember, looking for total utility, not terminal utility. Harder. :/
        + find max outcome outside of policy
        + find min outcome within policy - antagonistic teammate, minimizing EV

    Thoughts on comm strategies:
        - Exact
            + Timing is a consideration. Don't technically need to comm now if policy
              only at risk of changing later. Can delay comms until then.
                - As such, storing where policy changes gives the comm timing requirements
                - Also consider not communicating max potential change, but max change in current NEED of policy change
        - Given conditional relations between state-action pairs of teammate model
            + select states with best inference power over all affected states
        - Greedy (consider each independently, as in last paper)
        - Best pruning (as queried action makes other actions' subtrees irrelevant)
        - Limit communication content: limited states considered (what to communicate paper)
        - Limit computation: Heuristic: Util x Info x likelihood of state
        - Limit search space: Limiting to similar payoffs
        - Alternate strategy (like conditional value at risk): Maximizing expected util vs raise lower bound on util (safety value).
        - Alternate strategy: don't max payoff; rather, max chance agent made correct call (between two alternative distributions, maximize
        probability sampled outcome of one is greater than sampled outcome of other). In other words, maximize instances of regret, not ev of regret.
        - Query by committee - pick states where set of models most disagree; can select N most likely models (N most likely paths?)
        - Expected model change - pick queries that may most change policy (expectation over answers)

    Domains
        Managing 2+ teammates simultaneously.
            Helps with reviewer criticism
        Small domains:
            Multi-armed bandit (introduce conflict in same bandit? shared obs? someone explores while the other exploits?)
            Cooperative flocking (various roles?)
            Blocks world (tagged blocks which can only be moved by certain agent)
            Traditional pursuit?
            Generated MDP (Noa Agmon's work "Leading Ad Hoc Agents...")
        Human supervised classification
            -> setup: model selects action classifying item in state. Successive state is human classifying image (can skip?).
            -> Reward: S A S' -> util when select correct classification
            -> May need to adjust how immediate values are handled by nodes during planning


Nice to have, but completely beyond the scope of the current project.
    Profiling and unit tests

    Cython
        MDP planner?
        Domain scenarios - state transitions are likely a major factor

    Debug
        - use logging module http://inventwithpython.com/blog/2012/04/06/stop-using-print-for-debugging-a-5-minute-quickstart-guide-to-pythons-logging-module/

    Multiprocessing
        - far advanced, but consider doing something parallel. You know, maybe you should do an initial search until X
        branches, where X = #processors, then have each process handle a subtree. That's interesting. Probably not a huge
        gain, however. Doesn't keep the philosophy of put more resources into Y process because it is more promising. As
        X -> inf, you just have breadth first search.

    Hierarchical Graph Reduction (maybe keep for second PhD :P)
        Epsilon merging
        Multi-level graph partitioning


Modeling Notes:

    An interface for an agent model which can make state-based predictions as well as update with new information.

    We can take the state + operators approach. It works for state because all scenarios have a similar underlying
    representation: state features and values. But a model?

    Is it just a state-action mapping? No. It's a state x obs history -> action mapping. Just like a POMDP.
    Is dynamic programming relevant here, as with belief states?

    Is it a calculation? It can be, but it may be potentially computationally intensive.

    Should we cache the already computed components? How does a model update affect cached results?

    Is a model Markovian? 30 heads/70 tails -> 30% prediction, but 3/7 is updated very differently.

    In effect, a model is a distribution over possible policy instances. Communication highlights which instance we are in.