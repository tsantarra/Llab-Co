BY PRIORITY (ISH)

    TODO
        - DONE go over algorithm code
        - add pruning/heuristic selection to algorithm (comm scenario - actions)
        - go over communication process code (in comm scenario)
        - run small test
        - go over experimental setup
        - finalize changes and run first test!!!

    Tasks:
        - comms
            - test comm heuristics
            - full comm alg? pruned dec theory? YES

        - recipe sat
            - test domain to fix parameters
                - num recipes
                - num conditions
                - max length

        - experimental setups
            - vary cost of comm
            - vary cost of conflict
            - vary prior knowledge
            - vary num trials with same team (pre-comm)
            - vary heuristics
            - vary comm action space (with heuristics)
            - improving outcome for suboptimal team policy? (like Noa's work)
            - multiple teammates

        - planning optimization
            - can attempt backup operator optimization with little risk

    Data to log:
        Meta data (not operational data)
            - Parameters
                + Comm cost
                + Heuristic
                + Branching limit
                + Graph size (comm and policy)
                + Prior knowledge
            - Trial number
        Per run
            Per step
                - Expected util pre comm
                - Each comm
                - Each expected util after comm

    How:
        Docs: https://docs.python.org/3/howto/logging.html
        Cookbook: https://docs.python.org/3/howto/logging-cookbook.html
        Custom formatting: https://stackoverflow.com/questions/14844970/modifying-logging-message-format-based-on-message-logging-level-in-python3
        BaseRotatingHandler -> custom subclass to "rotate" between files (for different runs?)

        Tiers:
            - Debug: Dump any message (little/no format). Can be used for params (init) and odd outputs for... debugging.
            - Info: Reserved strictly for computational output.
            - Warning?
            - Error: For errors.
            - Critical?
        Handlers -
        Filters - each handler (and logger!) can filter by record level (warning/debug/etc) https://stackoverflow.com/questions/1383254/logging-streamhandler-and-standard-streams

    Planning note:
        - >1/3 of the planning time is spent calculating action values (during backup)
        - can reduce planning time by only completing a backup policy calculation every X iterations
        - in theory, this could be a huge speedup without losing asymptotic convergence

        - traverse nodes - incomplete successors calculations slow. I'm not sure this can be helped.

    Desires for experimental domain:
        - Multiple policies are successful
        - Scalable
        - Rearrangeable
            + add or remove policy dependencies
        - clear way to author/sample/learn teammate policies


    OSG Structure
        - fabfile logs in and performs remote commands
        - submit file specifies requirements for computation in condor network as well as what to run (.sh file)
        - .sh file loads modules/copies data and runs actual python files, converting output


    Experiment goals (Show ____):
        - exhaustive good, but infeasible
        - myopic bad
        - entropy okay (p log p)
        - variance in util better (hopefully)  (E[U' - E[U]])
        - weighting by likelihood (either) -> best   Ps (p log p)   ///   Ps (E[U' - E[U]])   (Ps = prob state)
        - if we had a paramaterized model, we could use Fisher information, but we don't (thank god)

        - more experience -> less comms?

    Active learning:
        - Fisher information? - dependent on model parameters (which we're not using a parameterized model)
        - Query by committee
            - useful for set of prior models
            - also works on N best (most likely) sequence labelings (using model predictions, generate the top most likely outcomes, use that as committee)

    Paper notes:
        - Search through hypothesis space. V subset of H - version space - set of hypotheses which are
            consistent with the observed data. See Burr Settles stuff on active learning.
        - Similar to minimizing expected error of prediction (forward looking through queries), but with a diff objective function
        - Noa Agmon leading paper -> task is to find the best possible REACHABLE joint actions/plans
        - Definition of problem
            - decentralized MDP variant
            - the modeler's view (planning in world state x modeling space) / POMDP
                - focus on planning while learning, otherwise only vaguely justified
            - active vs passive learning
            - active learning in ad hoc teams -> sharing policy information
            - planning for active learning -> planning over policy knowledge
            - optimize not for prediction accuracy, but for utility maximization
            - applications within teams of multiple unknown agents
        - Theoretical aspects
            - conditions where policy sharing is necessary
            - combinatorial space
            - MDP structure
            - DAG
            - early termination
            - cost effect? - free comm -> perfect info -> perfect policy
            - not submodular; matroids don't help

    Considerations
        - Querying a state that is unreachable could inform many reachable policy states. Hmmm.
        - Subset queries aren't likely to be optimal (as query choice depends on answers), but can improve runtime and coverage
        - True EV of a policy is not submodular (no guarantees on greedy alg).
        - Matroids don't apply either, as responses matter.
        - It's still not quite clear how to handle global policy model vs policy instance
            + Narrowing a policy from a stochastic policy is probably easier than "learning", no? Just use info gain to discern.
                - Doesn't take into consideration utility and indifference.

    EARLY TERMINATION - technically optional, but possibly huge in impact.
        + remember, looking for total utility, not terminal utility. Harder. :/
        + find max outcome outside of policy
        + find min outcome within policy - antagonistic teammate, minimizing EV

    Thoughts on comm strategies:
        - Exact
            + Timing is a consideration. Don't technically need to comm now if policy
              only at risk of changing later. Can delay comms until then.
                - As such, storing where policy changes gives the comm timing requirements
                - Also consider not communicating max potential change, but max change in current NEED of policy change
        - Given conditional relations between state-action pairs of teammate model
            + select states with best inference power over all affected states
        - Greedy (consider each independently, as in last paper)
        - Best pruning (as queried action makes other actions' subtrees irrelevant)
        - Limit communication content: limited states considered (what to communicate paper)
        - Limit computation: Heuristic: Util x Info x likelihood of state
        - Limit search space: Limiting to similar payoffs
        - Alternate strategy (like conditional value at risk): Maximizing expected util vs raise lower bound on util (safety value).
        - Alternate strategy: don't max payoff; rather, max chance agent made correct call (between two alternative distributions, maximize
        probability sampled outcome of one is greater than sampled outcome of other). In other words, maximize instances of regret, not ev of regret.
        - Query by committee - pick states where set of models most disagree; can select N most likely models (N most likely paths?)
        - Expected model change - pick queries that may most change policy (expectation over answers)

    Domains
        Managing 2+ teammates simultaneously.
            Helps with reviewer criticism
        Small domains:
            Multi-armed bandit (introduce conflict in same bandit? shared obs? someone explores while the other exploits?)
            Cooperative flocking (various roles?)
            Blocks world (tagged blocks which can only be moved by certain agent)
            Traditional pursuit?
            Generated MDP (Noa Agmon's work "Leading Ad Hoc Agents...")
        Human supervised classification
            -> setup: model selects action classifying item in state. Successive state is human classifying image (can skip?).
            -> Reward: S A S' -> util when select correct classification
            -> May need to adjust how immediate values are handled by nodes during planning


Nice to have, but completely beyond the scope of the current project.
    Profiling and unit tests

    Cython
        MDP planner?
        Domain scenarios - state transitions are likely a major factor

    Debug
        - use logging module http://inventwithpython.com/blog/2012/04/06/stop-using-print-for-debugging-a-5-minute-quickstart-guide-to-pythons-logging-module/

    Multiprocessing
        - far advanced, but consider doing something parallel. You know, maybe you should do an initial search until X
        branches, where X = #processors, then have each process handle a subtree. That's interesting. Probably not a huge
        gain, however. Doesn't keep the philosophy of put more resources into Y process because it is more promising. As
        X -> inf, you just have breadth first search.

    Hierarchical Graph Reduction (maybe keep for second PhD :P)
        Epsilon merging
        Multi-level graph partitioning


Modeling Notes:

    An interface for an agent model which can make state-based predictions as well as update with new information.

    We can take the state + operators approach. It works for state because all scenarios have a similar underlying
    representation: state features and values. But a model?

    Is it just a state-action mapping? No. It's a state x obs history -> action mapping. Just like a POMDP.
    Is dynamic programming relevant here, as with belief states?

    Is it a calculation? It can be, but it may be potentially computationally intensive.

    Should we cache the already computed components? How does a model update affect cached results?

    Is a model Markovian? 30 heads/70 tails -> 30% prediction, but 3/7 is updated very differently.

    In effect, a model is a distribution over possible policy instances. Communication highlights which instance we are in.