

    State space coverage is increasingly a consideration for policy sampling strategies. The issue lies in the ratio
    of relevant states--those reachable and on the trajectory of a given policy--to irrelevant states (off-policy/unlikely
    states). The presence of irrelevant states greatly increases the variety of policies but at the cost of diluting
    information, as is the case in Cops and Robbers where ~10^47,000 unique policies exist, but only ~13 varieties of
    observed trajectories are at play. There's something to be said for meaningful differences in behavior via the
    likelihood and differences of the trajectories that result, not necessarily from the differences that exist at the
    state-action mapping level. I believe this falls under "behavior equivalence", as reported by one of Doshi's students,
    though I don't believe there was any really meaningful progress on that front in that small amount of work.

    We can, of course, alter how behaviors are sampled, cutting out fine-grained policy sampling for something that
    instead samples in the space of observable behaviors. We could forward sample trajectories of policies, perhaps
    only reachable states given one agent's fixed actions, and leave all other state-action pairs pruned out. This
    leads to yet another consideration for how the ad hoc agent models these predictions, then. We could leave such
    predictions unbound in the aggregate model, instead only relying on which agents HAVE visited such states.

    I think this tracks. Anything that would be off policy would have 0 probability anyway for arriving at the state.
    Therefore, it doesn't matter if they had one action or another. We can simply keep them as None or not contained at
    all (sparse data structure), which will help with equivalence. Thank god.

    So what does our CRP structure look like, then? I don't think we need to keep a dense matrix anymore. Instead of
    state index x policy index, we could do state -> sparse policy mapping. But what about queries? If we were to
    query a teammate about an off-policy state, what is the response? None? Do we sample an action? How does that
    affect the belief posterior? Anything we don't observe is updated with uniform posterior? Does that work? It doesn't
    rule it out, but it does bias the result a little more toward observed trajectories with the same action. Of course,
    that may be okay. How do we justify this, theoretically? Behavioral equivalence might cover this. We're really
    creating a model that covers all "behaviorally equivalent" model, as all other policy choices are irrelevant. It's
    kind of an abstraction over policies or a policy bucket.

    Ok, so the new process is as follows:
        - Sample only relevant states for policy. All other policy states are removed. (assume covered with enough coordination)
        - Policy likelihoods are updated uniformly for models of policies where no behavior was observed.
        - When querying a teammate, sample a full policy, so we can get real policy answers.
        - Do we add queried policy info to the table on update? I feel like we could (should?). It depends what we're testing.
            - If we're not learning on successive runs with queries, we can leave it off and just assume policy coverage.
            - If we are learning with successive queries, we should add individual models to the table.
            - We should make clear our process in the paper.

    ------

    It doesn't make sense to count the number of unique optimal policies over the entire state space, as the relevant
    or reachable portion of the state space is comparatively small. In a fully deterministic cops and robbers scenario
    with a horizon of X, only X states will ever be reached, despite a branching factor of 3-5 at every state.
        Horizon: 13
        States involved in optimal policies = 25 (1 shared root, 12x2 states beyond)
        Total states: 88,000+
        States reachable under optimal individual policy: 10,000+
        Total optimal policies: 8 x 10^47,238

    Recipe sat scenario generates an ungodly number of optimal policies. I should try to constrain that.
    Interestingly enough, sampling out from the space only gave me like 1k unique policies. I need to check hashing
    and equality on those policies. Something seems off.


    ------

    Thoughts on trajectories
    It is likely improper to model teammates as having no consideration of policy trajectory when selecting
    sub-policies, i.e. teammates likely consider other agents acting within the scene just as we are modeling
    them. That assumption only works in a very strict case: in a fully-observable multiagent MDP where all agents
    select policies identically.

    So where are we, then? An initial policy graph (tree, really) where every trajectory is represented.
        ~|A_j|^h            possible trajectories
        ~|A_j|^(h+1)        possible queries
        ~|A_m||A_j|^(h+1)   possible query-reply pairs at the communication policy root, |A_m| = teammate actions

    One benefit: each query response prunes |A_m|-1 subtrees from the policy graph. This benefit is great the closer
    to the current state the query trajectory state occurs. For example, querying the current state can effectively
    reduce the space of trajectories as well as the space of possible queries by a factor of |A_j|. In general,
    querying a trajectory state at time t < h reduces the space of trajectories to
        |A_j|^h - (|A_j| - 1) * |A_j|^(h-t-1)
    and the space of queries to
        |A_j|^(h+1) - (|A_j| - 1) * |A_j|^(h-t).    # check this

    Can we then reason about the space of query/information states? Pruning was not considered substation before,
    as we allowed for multiple trajectories to arrive at the same state, which meant pruning in an abstract sense
    was specific to the domain and never guaranteed. In this new perspective, pruning IS guaranteed, and should be
    considered. Take, for example, a top-down communication strategy. It can be clearly shown that it requires
    no more than h queries to reach a complete plan (assuming no stochasticity). From a bottom-up approach, the
    query space required is exhaustive (assuming no model update). The real savings, then, can come from model
    updates, as they have the potential to prune branches of possibility anywhere within the policy graph. But
    calculating this potential for pruning would require a global policy analysis, which again is computationally
    taxing. Active learning may have some method of predicting changes in global prediction variance which may be
    useful here. Otherwise, we are at best estimating that heuristics like local information entropy can have
    large impacts across the model's ability to predict the teammate policy accurately.

    So, in summary, we should select heuristics that
        (1) prune as much of the trajectory state space as possible
            - bias heuristics toward short term trajectories or at least trajectory states with large subtrees
            - consider a heuristic measure of expected pruning (we can do that now!)
        (2) affect the global potential for pruning by having large information changes within the predictive model
            - this is where heuristics like information entropy are relevant
        (3) consider the effect on expected utility
            - relevant for in highly stochastic environments, as pruning is less likely

    Variance in utility addresses (2) and (3) to some degree. λ^t can help with (1), though not in cases where λ=1.
    This may be an argument for including p(s|π), as it further narrows the search space and biases heuristic values
    to trajectory states in the near future. Of course, it ignores off-policy trajectories, but given a response
    that lowers the expected utility of a trajectory sufficiently, our policy would change, resulting in a new
    set of trajectory states.

    Does it make sense to consider a policy response to a trajectory that violates my personal policy, e.g. "what
    will you do if we do A->B->C?" when I don't plan to do B? Probably not, if my policy is fixed. Can we process
    a noop ("not in policy") response to our benefit? Yes. We can prune the entire policy subgraph from
    consideration, as we know that any fixed policy that leads to that trajectory state is invalid (drop those
    models). Model update: all policies leading to trajectory state are dropped (p(r|q)=0); all other policies are
    equally likely to have made that response (p(r|q)=1).

    Conversely, what happens if we query a trajectory and get a valid response? Does that confirm the current
    trajectory for the teammate policy? Do we set the probabilities of all models of opposing trajectories to 0?
    I'd think so, yes. And the remaining policies are all equally likely to generate the trajectory but not equally
    likely to generate the response.

    Given these two model update considerations, You could imagine a heuristic that selected trajectory states that
    most closely bisected the set/probabilities of the remaining teammate policies. Ex:
        Model   Expected Trajectory
        1       A-B-C
        2       A-B-C'
        3       A-B-C''
        4       A'-B-C
        5       A'-B'-C
        6       A''-B-C
                            Eliminations
        Possible queries    (in policy & r)  (not in policy)     E[elim]     p(r) log p(r)
        A                   3/6 (B,B,B)      3/6 (3/6)           18/36       3*1/6 log 1/6 + 3/6 log 3/6 = 1.2424533248940002*
        A'                  5/6 (B,B')       2/6 (4/6)           18/36       2*1/6 log 1/6 + 4/6 log 4/6 = 0.8675632284814612
        A-B                 5/6 (C,C',C'')   3/6 (3/6)           24/36*      3*1/6 log 1/6 + 3/6 log 3/6 = 1.2424533248940002*
        A'-B                5/6 (C)          1/6 (5/6)           10/36       1/6 log 1/6 + 5/6 log 5/6   = 0.45056120886630463
        A'-B'               5/6 (C)          1/6 (5/6)           10/36       1/6 log 1/6 + 5/6 log 5/6   = 0.45056120886630463

        for policy in policies:
            response = get_response(policy)
            probs[response] += pr(policy)       # response = (Yes, action) or (No)

        Exp[elim] = sum(prob * (1-prob) for prob in probs.values)

    ------

    brainstorming on scenario with one composite policy (actions and comms planned together)
    new scenario would have the following changes:
        actions now includes query actions (no state change, but model change (part of state rep))
            except when world state denies it (agents in separate places, for instance)
        early termination no longer holds, as it may need to query still to ensure good policy in future states where comms prevented
        early termination, then, is really just a pruning eval for mixed scenarios with completely open comms
        in open comms, prove continued comms end value <= early termination end value
            basically show at that point, you've settled on the immediate action
                if the action result is deterministic, they are equal in resulting value
                otherwise, possibly save on comms from stochastic transition (avoid a state)
                (only works for constant comm cost)
        and the branching factor together makes the mdp just so big!
        and it doesn't have to be comms/no comms, really you could adjust comm cost (high cost = impossible)




